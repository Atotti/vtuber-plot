import os
import dotenv
import numpy as np
from typing import List
# æ–°ã—ã„ OpenAI Python ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ã‚’ä½¿ç”¨
# pip install --upgrade openai
from openai import OpenAI

from src import vtuber, utils

def calc_embeddings():
    # .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ï¼ˆOPENAI_API_KEY ãªã©ï¼‰
    dotenv.load_dotenv()
    # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    client = OpenAI()

    vtubers_json_path = "data/filtered_vtubers.json"
    vtubers_data = vtuber.load_vtubers(vtubers_json_path)

    print(f"ğŸ Loaded {len(vtubers_data)} vtubers!")

    names: List[str] = []
    sentences: List[str] = []

    # åŸ‹ã‚è¾¼ã¿ã®ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€
    output_dir = "data/text-embedding-3-large"
    os.makedirs(output_dir, exist_ok=True)

    for v in vtubers_data:
        name = utils.sanitize_path(v.name)
        save_path = os.path.join(output_dir, f"{name}.npy")

        # æ—¢ã«åŸ‹ã‚è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(save_path):
            print(f"âš ï¸  Embedding file already exists: {save_path}. Skipping.")
            continue

        # å¯¾å¿œã™ã‚‹ .md ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        md_path = f"data/SearchGPT/{name}.md"
        try:
            with open(md_path, "r", encoding="utf-8") as f:
                content = f.read()
        except FileNotFoundError:
            print(f"âš ï¸  File not found: {md_path}")
            continue

        names.append(name)
        sentences.append(content)

    if not sentences:
        print("No new embeddings to calculate. Exiting.")
        return

    print("ğŸ«  Start calculating embeddings...")

    batch_size = 4

    for start_idx in range(0, len(sentences), batch_size):
        batch_sentences = sentences[start_idx : start_idx + batch_size]
        batch_names = names[start_idx : start_idx + batch_size]

        # OpenAI Embeddings API ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿å–å¾—
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=batch_sentences
        )

        # response ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãªã®ã§ã€response.data ã‹ã‚‰å–ã‚Šå‡ºã™
        for i, name in enumerate(batch_names):
            embedding_vector = response.data[i].embedding  # ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆ
            save_path = os.path.join(output_dir, f"{name}.npy")
            np.save(save_path, embedding_vector)
            print(f"ğŸ‘Œ Saved {save_path}")
